---
title: "Preliminary EDA"
author: "Tanya Wang"
output: 
    html_notebook:
    fig_height: 6
    fig_width: 10
---
---


## Introduction

### Research Question
#### **How do the number of languages spoken at home affect English speaking ability?**
I want to see how the number of languages and the type of language(s) spoken at home can affect a person's ability to speak English well. I have a couple data sources to analyze that, one based on counties in the US, one based on the states in the US, and one of the US (the nation) overall. I may look for more datasets to add so that there's a bit more "color" and makes the entire project more interesting.


## The Data

First we need clean the environment and then load the package(s) we need. There are some packages that requiring installing, but I have commented them out after I have installed it once. Below is a description of each package I loaded and what it is used for:

- **tidyverse:** this is a collection of R packages designed for data science. It includes ggplot2, dplyr, tidyr, and more that we can use for everyday data analyses.
- **readxl:** this package makes it easy to get data out of Excel and into R
- **naniar:** contains tools for exploring missing data structures with minimal deviation from the common workflows of ggplot & tidy data
- **datasets:** contains tons of data the user can load and use immediately
```{r}
# This cleans up the environment.
rm(list = ls())

# Install necessary packages.
# install.packages("naniar")

# This loads the packages we need.
library(tidyverse)
library(readxl)
library(naniar)
library(datasets)
```


### Loading the Data

Now we read in our data from the xls file. This one is for states in the US and contains 52, all in separate sheets (because it contains Puerto Rico as well). Let's start off with just one state.
```{r}
xls_data <- "C:/Users/GuaiGuai/Documents/R/STAT184_FinalProject/2009-2013-acs-lang-tables-state.xls"

# This returns the names of all the sheets in our xls file (for future reference).
excel_sheets(path = xls_data)
```
```{r}
# I specified which sheet I want by it's name and made sure to set the na argument because some of the cells are empty but represented by "--", rather than a blank cell. I will deal with the cells with letters in them later.
Alabama <- read_excel(path = xls_data, sheet = "Alabama", na = "--")

# Let's see what the data looks like at first so that we know what kind of cleaning we need to do.
View(Alabama)
head(Alabama)
```

### Clean the Data

```{r}
# I need to rename the variables (columns) first to sort out what I'm looking at.
names(Alabama)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

# Now I need to filter the data initially so I get my first couple of rows. I will filter out more rows after this.
Alabama <-
  Alabama %>%
  filter(row_number() > 5, row_number() < 144)

# I used replace_with_na from the naniar package to replace certain values with NA based on the descriptions for the data I received. This will make cleaning up data a bit easier later when I get rid of cases that contain all NA values.
Alabama <-
  Alabama %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

# Check to see if my cleaning did anything.
View(Alabama)
head(Alabama)
```
After the initial filtering, I still need to take out some rows that are just totals (which therefore make the data not tidy). For now, I will keep the cases that have all NA values.
```{r}
# Dropping specific rows
Alabama <-
  Alabama[-c(2, 4, 5, 7:9, 13, 15, 17, 21, 29, 33, 44, 55, 65, 66, 78, 88, 101, 103, 122, 132),]

# Check to see if I get the output I want
View(Alabama)
head(Alabama)
```
Now that we have finished that cleaning, we need to add a new variable to prep this data table for when we append other data tables to it.
```{r}
# I use mutate() to add a state variable so when we append data tables, we will know which data is from which state.
Alabama <-
  Alabama %>%
  mutate(state = "Alabama")

# Check the data table
View(Alabama)
```

### Variable Types

So now that we have the data cleaned up and in a form that we want and like, we need to check how the variables are stored and make necessary adjustments. We know that we want the variables "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", and "EnglishError" to be numerical, and the rest (language & state) to be categorical.
```{r}
str(Alabama)
```
Based on the code I just ran, all the variables are stored as characters, so we need to change some of them into numerical variable types.
```{r}
# I use as.numeric() to change the variable types.
Alabama <-
  Alabama %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

# Now I check to make sure the variable types have changed as necessary.
str(Alabama)
```
Now that we have gone through the explanations for one data table, I will be going through the same process for all the rest of the data tables (52 in all).

### Load & Clean the Rest of the Data

```{r}

```

### Append the Tables

Now that I have all the data tables, I will start appending them together in terms of region. From this wikipedia page ("https://en.wikipedia.org/wiki/File:Census_Regions_and_Division_of_the_United_States.svg"), I found a map of the regions and divisons that the Census utilizes for the United States.
```{r}
# example: JapanChina <- bind_rows(China, Japan)
```


### Other Data Source

```{r}
# This is the other data source I am using. This is a dataset in R that is a matrix with 50 rows and 8 columns giving statistics about the population, income, illteracy rate, life expantancy, murder rate, high school graduation percentage, days with frost, and land area for each state.
data(state.x77)

data(midwest)
#other data source?
```


## Brief Description & Exaplanation Of the Data & the Research
I knew I wanted to do research and analysis on languages, culture, or something in the medical field, but I decided to focus my search on languages. I was able to find this data from the Census Bureau (specifically this website: "https://www.census.gov/data/tables/2013/demo/2009-2013-lang-tables.html
"). It contains four separate xls documents that each contain multiple sheets of data. It is all describing detailed languages spoken at home and ability to speak English for the population over a 5 year period (from 2009 to 2013).   
From the description on the site, I saw that the the tables are available for the nation, each of the 50 states, plus Washington, D.C. and Puerto Rico, counties with 100,000 or more total population and 25,000 or more speakers of languages other than English & Spanish, as well as core-based statistical areas (metropolitan statistical areas & micropolitan statistical areas) with 100,000 or more total population and 25,000 or more speakers of langauges other than English & Spanish.   
The information is collected by the American Community Survey, which contains multi-year data used to list all languages spoken in the United States that were reported during the sample period. The data is maintained by the Census Bureau's application programming interface (API)
Unfortunately, the data is only a sample of the total population since the ACS did not sample the households where some other languages are spoken, or because the person filling out the survey didn't report some languages and/or possibiliity reported another language instead. The English-speaking ability variable is self-reported so it represents the person's own perception about his or her own ability. The ACS questionnaires are also usually completed by one household member, so it may not reflect the overall household.   
For the data I am using (the states), there are separate tables for each state. A case is usually represented by a single language (or a group of languages to generalize some more specific languages) in each state. The cases vary by the state, but since I am planning on appending the tables to each other, we can do some rough math. If there are 52 tables for states and each table has about 145 cases (languages), and if we append all those tables together, we'll get around 7,540 caes in total. Whether or not I will be appending all of the states may vary. I may split the states up by region (northeast, west, south, midwest, etc.) and append those tables together and then have separate tables for each region instead.   
Once I finish cleaning and appending the tables, there will be 6 variables, language (categorical), number of speakers (numerical), margin of error (numerical), speak English less than "very well" (numerical), second margin of error (numerical), and state (categorical). I plan to mainly focus on the language, number of speakers, speak English les than "very well", and state. I want to see if there are any patterns in the language being spoken affecting the English speaking ability (maybe by how similar the language is to English). I also want to compare these trends between states (or most likely regions) and see if there are similar trends or not.   
In combination with my other data source, state.x77 (loaded from R), I want to see if maybe there could also be correlations between these languages spoken, English ability, and income, illteracy, and graduation percentage (these last 3 are variables in the state.x77 dataset). I am still debating on whether I would like to use all this in combination with antoher R dataset (probably the midwest dataset in the ggplot2 package) or another outside data source. This will be something you might see being explored more in my final report.

## Preliminary Observations

### Plots & Summary Statistics
```{r}

```

