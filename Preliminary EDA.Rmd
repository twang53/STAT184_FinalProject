---
title: "Preliminary EDA"
author: "Tanya Wang"
output: 
    html_notebook:
    fig_height: 6
    fig_width: 10
---
---


## Introduction

### Research Question
#### **How do the number of languages spoken at home affect English speaking ability?**
I want to see how the number of languages and the type of language(s) spoken at home can affect a person's ability to speak English well. I have a couple data sources to analyze that, one based on counties in the US, one based on the states in the US, and one of the US (the nation) overall. I may look for more datasets to add so that there's a bit more "color" and makes the entire project more interesting.   
For now, I will focus my energy on the datasets by state since that in itself is a lot of data to clean, wrangle, and such.


## The Data

First we need clean the environment and then load the package(s) we need. There are some packages that requiring installing, but I have commented them out after I have installed it once. Below is a description of each package I loaded and what it is used for:

- **tidyverse:** this is a collection of R packages designed for data science. It includes ggplot2, dplyr, tidyr, and more that we can use for everyday data analyses.
- **readxl:** this package makes it easy to get data out of Excel and into R
- **naniar:** contains tools for exploring missing data structures with minimal deviation from the common workflows of ggplot & tidy data
- **datasets:** contains tons of data the user can load and use immediately
```{r}
# This cleans up the environment.
rm(list = ls())

# Install necessary packages.
# install.packages("naniar")

# This loads the packages we need.
library(tidyverse)
library(readxl)
library(naniar)
library(datasets)
```


### Loading the Data

Now we read in our data from the xls file. This one is for **states in the US and contains 52 data tables**, all in separate sheets (because it contains Puerto Rico as well). Let's start off with just one state.
```{r}
xls_data <- "C:/Users/GuaiGuai/Documents/R/STAT184_FinalProject/2009-2013-acs-lang-tables-state.xls"

# This returns the names of all the sheets in our xls file (for future reference).
excel_sheets(path = xls_data)
```

Let's start with an example **Alabama**. I will take you through my process of loading and cleaning the data.
```{r}
# I specified which sheet I want by it's name and made sure to set the na argument because some of the cells are empty but represented by "--", rather than a blank cell. I will deal with the cells with letters in them later.
Alabama <- read_excel(path = xls_data, sheet = "Alabama", na = "--")

# Let's see what the data looks like at first so that we know what kind of cleaning we need to do.
View(Alabama)
head(Alabama)
```


### Clean the Data

```{r}
# I need to rename the variables (columns) first to sort out what I'm looking at.
names(Alabama)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

# Now I need to filter the data initially so I get my first couple of rows. I will filter out more rows after this.
Alabama <-
  Alabama %>%
  filter(row_number() > 5, row_number() < 144)

# I used replace_with_na from the naniar package to replace certain values with NA based on the descriptions for the data I received. This will make cleaning up data a bit easier later when I get rid of cases that contain all NA values.

# According to the dataset, (D) stands for data withheld to avoid disclosure, (B) stands for iether no sample observations or too few sample observations were available to compute an estimate, and (X) stands for that the question does not apply.
Alabama <-
  Alabama %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

# Check to see if my cleaning did anything.
View(Alabama)
head(Alabama)
```

After the initial filtering, I still need to take out some rows that are just totals (which therefore make the data not tidy). I will also be dropping the cases that have NA values.
```{r}
# Dropping specific rows
Alabama <-
  Alabama[-c(2, 4, 5, 7:9, 13, 15, 17, 21, 29, 33, 44, 55, 65, 66, 78, 88, 101, 103, 122),]

#Drop the cases with NA values
Alabama <-
  Alabama %>%
  na.omit()

# Check to see if I get the output I want
View(Alabama)
head(Alabama)
```

Now that we have finished that cleaning, we need to add a new variable to prep this data table for when we append other data tables to it.
```{r}
# I use mutate() to add a state variable so when we append data tables, we will know which data is from which state.
Alabama <-
  Alabama %>%
  mutate(state = "Alabama")

# Check the data table
View(Alabama)
```


### Variable Types

So now that we have the data cleaned up and in a form that we want and like, we need to check how the variables are stored and make necessary adjustments. We know that **we want the variables "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", and "EnglishError" to be numerical**, and the rest (language & state) to be categorical.
```{r}
str(Alabama)
```

Based on the code I just ran, all the variables are stored as characters, so we need to change some of them into numerical variable types.
```{r}
# I use as.numeric() to change the variable types.
Alabama <-
  Alabama %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

# Now I check to make sure the variable types have changed as necessary.
str(Alabama)
```
Now that we have gone through the explanations for one data table (Alabama), I will be going through the same process for all the rest of the data tables (52 in all), but for the sake of this preliminary EDA, I will only go through enough for one or two regions so we can get a brief look at what I want to do. I will continue loading and cleaning all 52 tables for my final report.


### Load & Clean the Rest of the Data

**Alaska**
```{r}
Alaska <- read_excel(path = xls_data, sheet = "Alaska", na = "--")

names(Alaska)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Alaska <-
  Alaska %>%
  filter(row_number() > 5, row_number() < 165)

Alaska <-
  Alaska %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Alaska <-
  Alaska[-c(2, 4, 5, 7:9, 13, 16, 17, 20, 28, 32, 44, 50, 58, 59, 71, 84, 103, 105, 144),]

Alaska <-
  Alaska %>%
  na.omit()

Alaska <-
  Alaska %>%
  mutate(state = "Alaska")

View(Alaska)

Alaska <-
  Alaska %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Alaska)
```

**Hawaii**
```{r}
Hawaii <- read_excel(path = xls_data, sheet = "Hawaii", na = "--")

names(Hawaii)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Hawaii <-
  Hawaii %>%
  filter(row_number() > 5, row_number() < 158)

Hawaii <-
  Hawaii %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Hawaii <-
  Hawaii[-c(2, 4, 5, 7:9, 12, 14, 16, 20, 28, 31, 42, 50, 61, 62, 76, 88, 122, 124, 141),]

Hawaii <-
  Hawaii %>%
  na.omit()

Hawaii <-
  Hawaii %>%
  mutate(state = "Hawaii")

View(Hawaii)

Hawaii <-
  Hawaii %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Hawaii)
```


### Append the Tables

Now that I have some data tables, I will start appending them together in terms of region.   
From this wikipedia page ("https://en.wikipedia.org/wiki/File:Census_Regions_and_Division_of_the_United_States.svg"), I found a map of the regions and divisons that the Census utilizes for the United States. For the purposes of the preliminary EDA, I only have some of the data tables, so will only have one or two of the regions. For the final report, I will have all the data tables, and therefore all the regions
```{r}
Pacific <- bind_rows(Alaska, Hawaii) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
View(Pacific)
```


### Other Data Source

This is the other data source I am using. This is a dataset in R that is a matrix with 50 rows and 8 columns giving statistics about the population, income, illteracy rate, life expantancy, murder rate, high school graduation percentage, days with frost, and land area for each state.
```{r}
# Load the data, assign a new data frame
data(state)
stateData <- data.frame(state.x77)

# Check out what is in the data and what it looks like
head(stateData)
View(stateData)
```
I am still considering if I would like to use more data sources, possibly the "midwest" dataset in R or another data source from the Census Bureau. But for now, I think I have a good amount of data and lots of coding and analysis to do for the preliminary EDA.


## Brief Description & Exaplanation Of the Data & the Research

I knew I wanted to do research and analysis on languages, culture, or something in the medical field, but I decided to focus my search on languages. I was able to find this data from the Census Bureau (specifically this website: "https://www.census.gov/data/tables/2013/demo/2009-2013-lang-tables.html
"). It contains four separate xls documents that each contain multiple sheets of data. It is all describing detailed languages spoken at home and ability to speak English for the population over a 5 year period (from 2009 to 2013).   
From the description on the site, I saw that the the tables are available for the nation, each of the 50 states, plus Washington, D.C. and Puerto Rico, counties with 100,000 or more total population and 25,000 or more speakers of languages other than English & Spanish, as well as core-based statistical areas (metropolitan statistical areas & micropolitan statistical areas) with 100,000 or more total population and 25,000 or more speakers of langauges other than English & Spanish.   
The information is collected by the American Community Survey, which contains multi-year data used to list all languages spoken in the United States that were reported during the sample period. The data is maintained by the Census Bureau's application programming interface (API)
Unfortunately, the data is only a sample of the total population since the ACS did not sample the households where some other languages are spoken, or because the person filling out the survey didn't report some languages and/or possibiliity reported another language instead. The English-speaking ability variable is self-reported so it represents the person's own perception about his or her own ability. The ACS questionnaires are also usually completed by one household member, so it may not reflect the overall household.   
For the data I am using (the states), there are separate tables for each state. A case is usually represented by a single language (or a group of languages to generalize some more specific languages) in each state. The cases vary by the state, but since I am planning on appending the tables to each other, we can do some rough math. If there are 52 tables for states and each table has about 145 cases (languages), and if we append all those tables together, we'll get around 7,540 caes in total. Whether or not I will be appending all of the states may vary. I may split the states up by region (northeast, west, south, midwest, etc.) and append those tables together and then have separate tables for each region instead.   
Once I finish cleaning and appending the tables, there will be 6 variables, language (categorical), number of speakers (numerical), margin of error (numerical), speak English less than "very well" (numerical), second margin of error (numerical), and state (categorical). I plan to mainly focus on the language, number of speakers, speak English les than "very well", and state. I want to see if there are any patterns in the language being spoken affecting the English speaking ability (maybe by how similar the language is to English). I also want to compare these trends between states (or most likely regions) and see if there are similar trends or not.   
In combination with my other data source, state.x77 (loaded from R), I want to see if maybe there could also be correlations between these languages spoken, English ability, and income, illteracy, and graduation percentage (these last 3 are variables in the state.x77 dataset). I am still debating on whether I would like to use all this in combination with antoher R dataset (probably the midwest dataset in the ggplot2 package) or another outside data source. This will be something you might see being explored more in my final report.


## Preliminary Observations

### Plots & Summary Statistics

So first off, I ran the summary() function on the Pacific Region data to get a brief glimpse of what's going on with the data. Remember that this region only contains data for the states Alaska and Hawaii.
```{r}
summary(Pacific)
```
The summary function tells us that there are 128 total cases (because of the 128 languages). Minimum number of speakers for a language is 4 people, but the maximum is 58345 people, with an average of 3350.8 people per language.   
Just as a quick note, the margin of errors are the 90% error.

Let's see the stateData dataset using summary() as well.
```{r}
summary(stateData)
```
We can see a lot of very interesting data summarizing the data for each of the 50 states. Before I explore possible plots to show relationships and such, let's run some more summary statistics.

```{r}
glimpse(Pacific)
```
```{r}
glimpse(stateData)
```
Using the glimpse() funciton, we get the number or rows, number or columns, the variable names, and a brief look at what kind of data is under each variable. For my Pacific data table, there are 128 rows/cases and 6 columns/variables. I noticed before, and now, that the language names have periods in front of them, but I don't believe it will affect any further analysis, so I won't be changing those names to take out the period symbols.   
For the stateData loaded from the state.x77 R dataset, we see that there are 50 rows/cases and 8 columns/variables. All the variables contain numerical data, so that means only the data I created myself (i.e. Pacific) contains a mix of numerical and categorical variables.

**I want to try more data wrangling with the datasets once I have more data cleaned and ready to use for the final report. I was having some issues with using group_by() and summarize() for my Pacific dataset, so I need to also work through those issues before I submit my final project. The code below is not giving me what I want (total speakers for both states grouped by language). I thought I'd include this to show what I am struggling with right now.**
```{r}
Pacific %>%
  group_by(Language) %>%
  summarize(totalSpeakers = sum(NumberSpeakers))
```


**Now let's create some plots!**

```{r}
Pacific %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("Number of Speakers") +
  ylab("English Ability Not 'Very Well'") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```
First, I created a simple dot plot with a line that helps us recognize any patterns in the data for the Pacific Region. I plotted the Number of Speakers versus the English Ability variable and differentiated color based on the state. As you can see, if we have more states, then this plot could become very interesting. If the plot becomes too messy looking, we could even use faceting instead to differentiate the states.   
From this plot, we can see that for the Pacific Region, as the number of speakers of a language increases, so does the amount of people who claim their English speaking ability is not/less than "Very Well".

I really want to see what the data for Alabama by itself is like.
```{r}
Alabama %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell)) +
  geom_point() +
  geom_smooth() +
  xlab("Number of Speakers") +
  ylab("English Ability Not 'Very Well'") +
  labs(title = "Speakers vs English Ability in Alabama")
```
As we can see, a similar pattern is showing up, but there seems to be only one data point in the extreme of a large number of speakers and a large amount of people claiming their English ability is not "very well". This dot seems to be an outlier and could possibly be messing with our overall distribution.

Now let's move onto plots for stateData.
```{r}
stateData %>%
  ggplot(aes(x = Income, y = Life.Exp)) +
  geom_point() +
  geom_smooth() +
  ylab("Life Expectancy") +
  labs(title = "Income vs Life Expectancy for the United States")
```
The graph above is from the stateData dataset I am using. I thought it'd be cool to see if there's a trend for income versus life expectancy and it's really interesting. There're a couple outliers that make the data a bit weird, but overall, up to a certain point (around 4700 maybe?) for income, the higher it goes, the higher the life expectancy. Then we hit kind of a max and thne it starts plateauing and decreasing, meaning if the income keeps going higher, then the life expectancy doesn't increase and might even actually decrease. Note that these are average income and average life expectancy for each individual state, so it may not be extremely accurate since there are only 50 data points (1 for each state).


### Final Thoughts
Overall, based on how everything has gone from importing data, cleaning it, messing around with it to get it to look nicer, and then exploring some summary statistics and plots, I realize that maybe my research question needs to change a little bit as well. I may need to look for some more data sources so that I could possibly join different sources to get more interesting plots and such. Of course, this Preliminary EDA seems a little bit underwhelming, probably because I only imported and cleaned 3 out of the 52 data tables I have. I think once I import and clean up all the data tables and separate them into the regions (Pacific, West, Midwest, South, and Northeast), I might be able to run similar plots for each of the regions and compare those. Maybe I could even combine all the regions and create a graph to compare not only the states, but also the regions. That would be really cool.

I think I may have to change my research question to be more along the lines of **"Do the number of people who speak a language affect the English speaking ability differ for each state and/or region?"**

If I can figure out how to get the stateData in a form where "state" is also a variable that I can manipulate and use, then I could possibly join some tables to see the relationships between number of speakers for a specific language versus the population of a state, or something along those lines.

There is definitely a lot I need to discuss with the professor and TA in regards to my data wrangling commands not working, my data sources, and if I'm even going down the right path.