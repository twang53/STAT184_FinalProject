---
title: "Final Project"
author: "Tanya Wang"
output: 
    html_notebook:
      toc: true
      toc_float:
        collapsed: false
        smooth_scroll: true
      toc_depth: 2
      fig_height: 6
      fig_width: 10
---

# How Languages Spoken Affect English Ability
You can find this project and the datsets I used at the following github.io website: "https://twang53.github.io/STAT184_FinalProject/"

<br>

## Introduction

### Research Question
#### **How does the number of people who speak a langauge affect their English speaking ability?**   
I want to see how the number of peopel who speak a certain language at home can affect a person's ability to speak English well. If possible, I will also try seeing if there is a correlation between type of language and English speaking ability. This is really interesting for me because I've always been interested in the linguistics field and I thought it would be cool to see if there was some trend in the number of languages (types of languages) spoken at home versus self-reported English speaking ability. Ideally, we'd like a scientific output for Englihs speaking ability for each person based on statistical analysis of their responses to different tasks, but the data I got from the Census doesn't have that since this pattern I'm looking for wasn't what they originally collected the data for.   
From experience when I submitted my preliminary EDA, I found out that I had to significanctly dial down the number of data tables I wanted to use since the data is not formatted in a way that is easily usable by R. I decided to stick with only a couple of states from the datasets by state since that in itself is a lot of data to clean, wrangle, and such. I also decided to add in another dataset for the entire US nation itself to see if the same trend we might see in states may also be true for the entire nation as a whole.

<br>

## The Data

First we need to clean the environment and then load the package(s) we need. There are some packages that requiring installing, but I have commented them out after I have installed it once so that the code doesn't run but you can see what packages I needed to install. Below is a description of each package I loaded and what it is used for:

- **tidyverse:** this is a collection of R packages designed for data science. It includes ggplot2, dplyr, tidyr, and more that we can use for everyday data analyses.
- **readxl:** this package makes it easy to get data out of Excel and into R
- **naniar:** contains tools for exploring missing data structures with minimal deviation from the common workflows of ggplot & tidy data
- **datasets:** contains tons of data the user can load and use immediately
```{r, message = FALSE}
# This cleans up the environment.
rm(list = ls())

# Install necessary packages.
# install.packages("naniar")

# This loads the packages we need.
library(tidyverse)
library(readxl)
library(naniar)
library(datasets)
```

<br>

### Loading the Data

Now we read in our data from the xls file. This one is for **states in the US and contains 52 data tables**, all in separate sheets (because it contains Puerto Rico as well). This data was taken from the Census Bureau website.   
Let's start off with just one state.
```{r}
# Specifying Data Path (change path as needed since this is specific to where the data is saved on my laptop)
xls_data <- "C:/Users/GuaiGuai/Documents/R/STAT184_FinalProject/2009-2013-acs-lang-tables-state.xls"

# This returns the names of all the sheets in our xls file (for future reference).
excel_sheets(path = xls_data)
```

Let's start with an example: **Alabama**. I will take you through my process of loading and cleaning the data.
```{r, message = FALSE}
# I specified which sheet I want by it's name and made sure to set the na argument because some of the cells are empty but represented by "--", rather than a blank cell. I will deal with the cells with letters in them later.
Alabama <- read_excel(path = xls_data, sheet = "Alabama", na = "--")


# Let's see what the data looks like at first so that we know what kind of cleaning we need to do.

# View(Alabama) --> This code is commented out because View() should not be put in a markdown file and was a function I used in the console to check my data. I just wanted to add it here initially so you know I was checking my entire table. For the rest of the markdown file, I will not be including View() functions, even if I run them in the console.

head(Alabama)
```

<br>

### Clean the Data

```{r}
# I need to rename the variables (columns) first to sort out what I'm looking at.
names(Alabama)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

# Now I need to filter the data initially so I get my first couple of rows. I will filter out more rows after this.
Alabama <-
  Alabama %>%
  filter(row_number() > 5, row_number() < 144)


# I used replace_with_na from the naniar package to replace certain values with NA based on the descriptions for the data I received. This will make cleaning up data a bit easier later when I get rid of cases that contain all NA values.

# According to the dataset, (D) stands for data withheld to avoid disclosure, (B) stands for either no sample observations or too few sample observations were available to compute an estimate, and (X) stands for that the question does not apply.
Alabama <-
  Alabama %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

# Check to see if my cleaning did anything.
head(Alabama)
```

After the initial filtering, I still need to take out some rows that are just totals (which therefore make the data not tidy). I will also be dropping the cases that have NA values.
```{r}
# Dropping specific rows
Alabama <-
  Alabama[-c(2, 4, 5, 7:9, 13, 15, 17, 21, 29, 33, 44, 55, 65, 66, 78, 88, 101, 103, 122),]

#Drop the cases with NA values
Alabama <-
  Alabama %>%
  na.omit()

# Check to see if I get the output I want
head(Alabama)
```

Now that we have finished that cleaning, we need to add a new variable to prep this data table for when we append other data tables to it.
```{r}
# I use mutate() to add a state variable so when we append data tables, we will know which data is from which state.
Alabama <-
  Alabama %>%
  mutate(state = "Alabama")

# Check the data table
names(Alabama)
```

<br>

### Variable Types

So now that we have the data cleaned up and in a form that we want and like, we need to check how the variables are stored and make necessary adjustments. We know that **we want the variables "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", and "EnglishError" to be numerical**, and the rest (language & state) to be categorical.
```{r}
str(Alabama)
```

Based on the code I just ran, all the variables are stored as characters, so we need to change some of them into numerical variable types.
```{r}
# I use as.numeric() to change the variable types.
Alabama <-
  Alabama %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

# Now I check to make sure the variable types have changed as necessary.
str(Alabama)
```

Now that we have gone through the explanations for one data table (Alabama), I will be going through the same process for specific states so that I have enough data for some individual states that I can combine to represent the other regions.

<br>

**Technical Challenge:** I had previously considered cleaning all 52 tables, but after discussing with the TA, we decided it would be more beneficial and realistic to only choose a couple of the tables, since the data required some hard-coding to be done via the excel spreadsheet before importing into R (I had to go in and delete a bunch of periods that were showing up in the beginning of language names that were causing me issues when trying to wrangle my data and use join and certain reduction/transformation functions).

<br>

### Load & Clean the Rest of the Data
**NOTE:** Note that I occasionally used the View() function in the console to check my code, that's why I don't have any inspecting functions for the data tables below. I believe the step-by-step process I walked you through previously is sufficient evidence that I constantly check my data after every step to make sure everything is okay.

<br>

#### Pacific Region
**Alaska**
```{r, message = FALSE}
Alaska <- read_excel(path = xls_data, sheet = "Alaska", na = "--")

names(Alaska)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Alaska <-
  Alaska %>%
  filter(row_number() > 5, row_number() < 165)

Alaska <-
  Alaska %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Alaska <-
  Alaska[-c(2, 4, 5, 7:9, 13, 16, 17, 20, 28, 32, 44, 50, 58, 59, 71, 84, 103, 105, 144),]

Alaska <-
  Alaska %>%
  na.omit()

Alaska <-
  Alaska %>%
  mutate(state = "Alaska")


Alaska <-
  Alaska %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Alaska)
```

**Hawaii**
```{r, message = FALSE}
Hawaii <- read_excel(path = xls_data, sheet = "Hawaii", na = "--")

names(Hawaii)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Hawaii <-
  Hawaii %>%
  filter(row_number() > 5, row_number() < 158)

Hawaii <-
  Hawaii %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Hawaii <-
  Hawaii[-c(2, 4, 5, 7:9, 12, 14, 16, 20, 28, 31, 42, 50, 61, 62, 76, 88, 122, 124, 141),]

Hawaii <-
  Hawaii %>%
  na.omit()

Hawaii <-
  Hawaii %>%
  mutate(state = "Hawaii")


Hawaii <-
  Hawaii %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Hawaii)
```

**Utah**
```{r, message = FALSE}
Utah <- read_excel(path = xls_data, sheet = "Utah", na = "--")

names(Utah)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Utah <-
  Utah %>%
  filter(row_number() > 5, row_number() < 169)

Utah <-
  Utah %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Utah <-
  Utah[-c(2, 4, 5, 7:9, 12, 14, 17, 22, 29, 33, 44, 52, 64, 65, 79, 91, 115, 117, 155),]

Utah <-
  Utah %>%
  na.omit()

Utah <-
  Utah %>%
  mutate(state = "Utah")


Utah <-
  Utah %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Utah)
```

<br>

#### Midwest Region
**Iowa**
```{r, message = FALSE}
Iowa <- read_excel(path = xls_data, sheet = "Iowa", na = "--")

names(Iowa)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Iowa <-
  Iowa %>%
  filter(row_number() > 5, row_number() < 161)

Iowa <-
  Iowa %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Iowa <-
  Iowa[-c(2, 4:6, 12, 14, 18, 22, 30, 34, 45, 54, 64, 65, 78, 91, 108, 110, 133, 149),]

Iowa <-
  Iowa %>%
  na.omit()

Iowa <-
  Iowa %>%
  mutate(state = "Iowa")


Iowa <-
  Iowa %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Iowa)
```

**Michigan**
```{r, message = FALSE}
Michigan <- read_excel(path = xls_data, sheet = "Michigan", na = "--")

names(Michigan)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Michigan <-
  Michigan %>%
  filter(row_number() > 5, row_number() < 185)

Michigan <-
  Michigan %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Michigan <-
  Michigan[-c(2, 4:7, 13, 15, 18, 23, 31, 35, 48, 62, 76, 77, 91, 111, 132, 134, 157, 172, 175),]

Michigan <-
  Michigan %>%
  na.omit()

Michigan <-
  Michigan %>%
  mutate(state = "Michigan")


Michigan <-
  Michigan %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Michigan)
```

**Ohio**
```{r, message = FALSE}
Ohio <- read_excel(path = xls_data, sheet = "Ohio", na = "--")

names(Ohio)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Ohio <-
  Ohio %>%
  filter(row_number() > 5, row_number() < 187)

Ohio <-
  Ohio %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Ohio <-
  Ohio[-c(2, 4, 5, 6, 11, 14, 18, 22, 30, 34, 47, 60, 73, 74, 89, 107, 124, 126, 155, 171),]

Ohio <-
  Ohio %>%
  na.omit()

Ohio <-
  Ohio %>%
  mutate(state = "Ohio")


Ohio <-
  Ohio %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Ohio)
```

<br>

#### South Region
**Texas**
```{r, message = FALSE}
Texas <- read_excel(path = xls_data, sheet = "Texas", na = "--")

names(Texas)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Texas <-
  Texas %>%
  filter(row_number() > 5, row_number() < 228)

Texas <-
  Texas %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Texas <-
  Texas[-c(2, 4:7, 13, 16, 20, 24, 32, 36, 48, 63, 66, 80, 81, 95, 117, 145, 147, 157, 192, 210, 213),]

Texas <-
  Texas %>%
  na.omit()

Texas <-
  Texas %>%
  mutate(state = "Texas")


Texas <-
  Texas %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Texas)
```

**Florida**
```{r, message = FALSE}
Florida <- read_excel(path = xls_data, sheet = "Florida", na = "--")

names(Florida)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Florida <-
  Florida %>%
  filter(row_number() > 5, row_number() < 200)

Florida <-
  Florida %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Florida <-
  Florida[-c(2, 4:7, 13, 16, 20, 25, 33, 37, 50, 63, 76, 77, 91, 109, 132, 134, 166, 181),]

Florida <-
  Florida %>%
  na.omit()

Florida <-
  Florida %>%
  mutate(state = "Florida")


Florida <-
  Florida %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Florida)
```

**Virginia**
```{r, message = FALSE}
Virginia <- read_excel(path = xls_data, sheet = "Virginia", na = "--")

names(Virginia)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Virginia <-
  Virginia %>%
  filter(row_number() > 5, row_number() < 194)

Virginia <-
  Virginia %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Virginia <-
  Virginia[-c(2, 4:6, 12, 14, 17, 22, 31, 35, 48, 61, 75, 76, 90, 108, 131, 133, 160, 178),]

Virginia <-
  Virginia %>%
  na.omit()

Virginia <-
  Virginia %>%
  mutate(state = "Virginia")


Virginia <-
  Virginia %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Virginia)
```

<br>

#### Northeast Region
**Pennsylvania**
FINISH

**Vermont**
FINISH

**Maine**
FINISH

<br>

### Append the Tables

Now that I have the individual data tables for states, I will start appending them together in terms of region.   
From this wikipedia page ("https://en.wikipedia.org/wiki/File:Census_Regions_and_Division_of_the_United_States.svg"), I found a map of the regions and divisons that the Census utilizes for the United States.

**Pacific Region:** Only Alaska, Hawaii, and Utah
```{r}
Pacific <- bind_rows(Alaska, Hawaii, Utah) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(Pacific)
```

**Midwest Region:** Only Iowa, Michigan, and Ohio
```{r}
Midwest <- bind_rows(Iowa, Michigan, Ohio) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(Midwest)
```

**South Region:** Only Texas, Florida, and Virginia
```{r}
South <- bind_rows(Texas, Florida, Virginia) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(South)
```

**Northeast Region:** Only Pennsylvania, Vermont, and Maine
```{r}
Northeast <- bind_rows(Pennsylvania, Vermont, Maine) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(Northeast)
```

<br>

### Other Data Source

This is the other data source I am using. It is also from the Census Bureau, but instead of language data for each state, it is for the nation as a whole. The data for each state would count as a subset of this data, so I wanted to know if the patterns I observe for the states/regions are also reflected in the overall nation data.
```{r, message = FALSE}
# Specifying Data Path (change path as needed since this is specific to where the data is saved on my laptop)
xls_nationData <- "C:/Users/GuaiGuai/Documents/R/STAT184_FinalProject/2009-2013-acs-lang-tables-nation.xls"

# This returns the names of all the sheets in our xls file (for future reference).
excel_sheets(path = xls_nationData)

# Name the data
Nation <- read_excel(path = xls_nationData, na = "--")

# Let's take a look at the data
head(Nation)
```

The data doesn't look too great, so we need to do some cleaning. I followed the same process I used for cleaning the states data tables, except I don't need the mutate() function to add a new variable for state since this data table is for the entire nation (every state and territory in the United States of America).
```{r}
names(Nation)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Nation <-
  Nation %>%
  filter(row_number() > 5, row_number() < 377)

Nation <-
  Nation %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Nation <-
  Nation[-c(2, 4:8, 14, 17, 21, 26, 35, 39, 53, 68, 89, 90, 106, 132, 175, 177, 331, 352, 356),]

Nation <-
  Nation %>%
  na.omit()


Nation <-
  Nation %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Nation)
```

<br>

## Brief Description & Exaplanation Of the Data & the Research
CHANGE PARAGRAPHS BELOW

I knew I wanted to do research and analysis on languages, culture, or something in the medical field, but I decided to focus my search on languages. I was able to find this data from the Census Bureau (specifically this website: "https://www.census.gov/data/tables/2013/demo/2009-2013-lang-tables.html
"). It contains four separate xls documents that each contain multiple sheets of data. It is all describing detailed languages spoken at home and ability to speak English for the population over a 5 year period (from 2009 to 2013).   
From the description on the site, I saw that the the tables are available for the nation, each of the 50 states, plus Washington, D.C. and Puerto Rico, counties with 100,000 or more total population and 25,000 or more speakers of languages other than English & Spanish, as well as core-based statistical areas (metropolitan statistical areas & micropolitan statistical areas) with 100,000 or more total population and 25,000 or more speakers of langauges other than English & Spanish.   
The information is collected by the American Community Survey, which contains multi-year data used to list all languages spoken in the United States that were reported during the sample period. The data is maintained by the Census Bureau's application programming interface (API)
Unfortunately, the data is only a sample of the total population since the ACS did not sample the households where some other languages are spoken, or because the person filling out the survey didn't report some languages and/or possibiliity reported another language instead. The English-speaking ability variable is self-reported so it represents the person's own perception about his or her own ability. The ACS questionnaires are also usually completed by one household member, so it may not reflect the overall household.   
For the data I am using (the states), there are separate tables for each state. A case is usually represented by a single language (or a group of languages to generalize some more specific languages) in each state. The cases vary by the state, but since I am planning on appending the tables to each other, we can do some rough math. If there are 52 tables for states and each table has about 145 cases (languages), and if we append all those tables together, we'll get around 7,540 caes in total. Whether or not I will be appending all of the states may vary. I may split the states up by region (northeast, west, south, midwest, etc.) and append those tables together and then have separate tables for each region instead.   
Once I finish cleaning and appending the tables, there will be 6 variables, language (categorical), number of speakers (numerical), margin of error (numerical), speak English less than "very well" (numerical), second margin of error (numerical), and state (categorical). I plan to mainly focus on the language, number of speakers, speak English les than "very well", and state. I want to see if there are any patterns in the language being spoken affecting the English speaking ability (maybe by how similar the language is to English). I also want to compare these trends between states (or most likely regions) and see if there are similar trends or not.   
In combination with my other data source, state.x77 (loaded from R), I want to see if maybe there could also be correlations between these languages spoken, English ability, and income, illteracy, and graduation percentage (these last 3 are variables in the state.x77 dataset). I am still debating on whether I would like to use all this in combination with antoher R dataset (probably the midwest dataset in the ggplot2 package) or another outside data source. This will be something you might see being explored more in my final report.

<br>

## Data Wrangling
Before we dive into data wrangling, let's take a look at some summary statistics.

### Summary Statistics
So first off, I ran the summary() and glimpse() functions on the Pacific Region data to get a brief glimpse of what's going on with the data. Remember that each region I created only contain 3 out of all the states in that region due to how much hard-coding and cleaning I would have to do if I imported all 52 tables for the states plus Puerto Rico.
```{r}
summary(Pacific)
```
The summary function tells us that there are 206 total cases (because of the 206 languages). Minimum number of speakers for a language is 4 people, but the maximum is 245947 people, with an average of 3862.9 people per language.   
Just as a quick note, the margin of errors are the 90% error.

```{r}
glimpse(Pacific)
```
Using the glimpse() funciton, we get the number or rows, number or columns, the variable names, and a brief look at what kind of data is under each variable. For my Pacific data table, there are 206 rows/cases and 6 columns/variables. In my preliminary EDA, I had noticed there were periods in front of my language names. After discussing with the TA, I ended up taking those out manully within the excel spreadsheet (basically hard-coding the data) so that I wouldn't have to worry about taking it out in R Studio.

Now, let's take a look at each of the other regions. I only used the summary() function since you get the general gist of what glimpse() does and there's no need to use that function for every other region as well.
```{r}
summary(Midwest)
```
EXPLANATION

```{r}
summary(South)
```
EXPLANATION

```{r}
summary(Northeast)
```
EXPLANATION

After looking at the regions, let's take a look at the overall nation. I decided to use both summary() and glimpse() since this dataset is a bit different than the previous datasets for the regions.
```{r}
summary(Nation)
```
EXPLANATION

```{r}
glimpse(Nation)
```
EXPLANATION

### **Now let's get started with data wrangling.**   
I started off with a simple query that I had an issue getting to work properly in my preliminary EDA. I basically take my Pacific data table and group it by the language, then summarize it to sum up the total number of speakers for each langauge. Based on the output below, my code works!   
As you can see, my output is arranged in alphabetical order by language name, even though I didn't specify anything with the arr() function.
```{r}
Pacific %>%
  group_by(Language) %>%
  summarize(totalSpeakers = sum(NumberSpeakers))
```

Let's try some other code!
```{r}

```

<br>

## Data Visualization

### Plots
Let's first see if there's a relationship between the number of speakers of a language and their English ability. I created a simple dot plot with a line that helps us recognize any patterns in the data for the Pacific Region. I plotted the Number of Speakers versus the English Ability variable and differentiated color based on the state. As you can see from the output, if we have more than three states, then this plot could become very interesting. If the plot becomes too messy looking, we could even use faceting instead to differentiate the states.   
From this plot, we can see that for the Pacific Region, as the number of speakers of a language increases, so does the amount of people who claim their English speaking ability is not/less than "Very Well".
```{r}
Pacific %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```

But it's hard to see the data for Alaska since Utah has a huge outlier, so what if we limited the x-axis and y-axis ranges to take out the outlier?
```{r}
Pacific %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlim(0, 60000) +
  ylim(0, 40000) +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```
EXPLANATION

What about for the other regions?
```{r}
Midwest %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```

```{r}
South %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```

```{r}
Northeast %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```

```{r}
Nation %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```

```{r}
Pacific %>%
  ggplot(aes(Language)) +
  geom_bar() +
  facet_grid(state ~ .)
```

<br>

## Conclusion/Final Thoughts




