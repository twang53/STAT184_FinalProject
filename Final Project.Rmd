---
title: "Final Project"
author: "Tanya Wang"
output: 
    html_notebook:
      toc: true
      toc_float:
        collapsed: false
        smooth_scroll: true
      toc_depth: 2
      fig_height: 6
      fig_width: 10
---

# How Languages Spoken Affect English Ability
You can find this project and the datsets I used at the following github.io website: "https://twang53.github.io/STAT184_FinalProject/"

<br>

## Introduction

### Research Question
#### **How does the number of people who speak a langauge affect their English speaking ability?**   
I want to see how the number of peopel who speak a certain language at home can affect a person's ability to speak English well. If possible, I will also try seeing if there is a correlation between type of language and English speaking ability. This is really interesting for me because I've always been interested in the linguistics field and I thought it would be cool to see if there was some trend in the number of languages (types of languages) spoken at home versus self-reported English speaking ability. Ideally, we'd like a scientific output for Englihs speaking ability for each person based on statistical analysis of their responses to different tasks, but the data I got from the Census doesn't have that since this pattern I'm looking for wasn't what they originally collected the data for.   

From experience when I submitted my preliminary EDA, I found out that I had to significanctly dial down the number of data tables I wanted to use since the data is not formatted in a way that is easily usable by R. I decided to stick with only a couple of states from the datasets by state since that in itself is a lot of data to clean, wrangle, and such. I also decided to add in another dataset for the entire US nation itself to see if the same trend we might see in states may also be true for the entire nation as a whole.

<br>

## The Data

First we need to clean the environment and then load the package(s) we need. There are some packages that requiring installing, but I have commented them out after I have installed it once so that the code doesn't run but you can see what packages I needed to install. Below is a description of each package I loaded and what it is used for:

- **tidyverse:** this is a collection of R packages designed for data science. It includes ggplot2, dplyr, tidyr, and more that we can use for everyday data analyses.
- **readxl:** this package makes it easy to get data out of Excel and into R
- **naniar:** contains tools for exploring missing data structures with minimal deviation from the common workflows of ggplot & tidy data
- **datasets:** contains tons of data the user can load and use immediately

```{r, message = FALSE}
# This cleans up the environment.
rm(list = ls())

# Install necessary packages.
# install.packages("naniar")

# This loads the packages we need.
library(tidyverse)
library(readxl)
library(naniar)
library(datasets)
```

<br>

### Loading the Data

Now we read in our data from the xls file. This one is for **states in the US and contains 52 data tables**, all in separate sheets (because it contains Puerto Rico as well). This data was taken from the Census Bureau website.   
Let's start off with just one state.
```{r}
# Specifying Data Path (change path as needed since this is specific to where the data is saved on my laptop)
xls_data <- "C:/Users/GuaiGuai/Documents/R/STAT184_FinalProject/2009-2013-acs-lang-tables-state.xls"

# This returns the names of all the sheets in our xls file (for future reference).
excel_sheets(path = xls_data)
```

Let's start with an example: **Alabama**. I will take you through my process of loading and cleaning the data.
```{r, message = FALSE}
# I specified which sheet I want by it's name and made sure to set the na argument because some of the cells are empty but represented by "--", rather than a blank cell. I will deal with the cells with letters in them later.
Alabama <- read_excel(path = xls_data, sheet = "Alabama", na = "--")


# Let's see what the data looks like at first so that we know what kind of cleaning we need to do.

# View(Alabama) --> This code is commented out because View() should not be put in a markdown file and was a function I used in the console to check my data. I just wanted to add it here initially so you know I was checking my entire table. For the rest of the markdown file, I will not be including View() functions, even if I run them in the console.

head(Alabama)
```

<br>

### Clean the Data

```{r}
# I need to rename the variables (columns) first to sort out what I'm looking at.
names(Alabama)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

# Now I need to filter the data initially so I get my first couple of rows. I will filter out more rows after this.
Alabama <-
  Alabama %>%
  filter(row_number() > 5, row_number() < 144)


# I used replace_with_na from the naniar package to replace certain values with NA based on the descriptions for the data I received. This will make cleaning up data a bit easier later when I get rid of cases that contain all NA values.

# According to the dataset, (D) stands for data withheld to avoid disclosure, (B) stands for either no sample observations or too few sample observations were available to compute an estimate, and (X) stands for that the question does not apply.
Alabama <-
  Alabama %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

# Check to see if my cleaning did anything.
head(Alabama)
```

After the initial filtering, I still need to take out some rows that are just totals (which therefore make the data not tidy). I will also be dropping the cases that have NA values.
```{r}
# Dropping specific rows
Alabama <-
  Alabama[-c(2, 4, 5, 7:9, 13, 15, 17, 21, 29, 33, 44, 55, 65, 66, 78, 88, 101, 103, 122),]

#Drop the cases with NA values
Alabama <-
  Alabama %>%
  na.omit()

# Check to see if I get the output I want
head(Alabama)
```

Now that we have finished that cleaning, we need to add a new variable to prep this data table for when we append other data tables to it.
```{r}
# I use mutate() to add a state variable so when we append data tables, we will know which data is from which state.
Alabama <-
  Alabama %>%
  mutate(state = "Alabama")

# Check the data table
names(Alabama)
```

<br>

### Variable Types

So now that we have the data cleaned up and in a form that we want and like, we need to check how the variables are stored and make necessary adjustments. We know that **we want the variables "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", and "EnglishError" to be numerical**, and the rest (language & state) to be categorical.
```{r}
str(Alabama)
```

Based on the code I just ran, all the variables are stored as characters, so we need to change some of them into numerical variable types.
```{r}
# I use as.numeric() to change the variable types.
Alabama <-
  Alabama %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

# Now I check to make sure the variable types have changed as necessary.
str(Alabama)
```

Now that we have gone through the explanations for one data table (Alabama), I will be going through the same process for specific states so that I have enough data for some individual states that I can combine to represent the other regions.

<br>

**Technical Challenge:** I had previously considered cleaning all 52 tables, but after discussing with the TA, we decided it would be more beneficial and realistic to only choose a couple of the tables, since the data required some hard-coding to be done via the excel spreadsheet before importing into R (I had to go in and delete a bunch of periods that were showing up in the beginning of language names that were causing me issues when trying to wrangle my data and use join and certain reduction/transformation functions).

<br>

### Load & Clean the Rest of the Data
**NOTE:** Note that I occasionally used the View() function in the console to check my code, that's why I don't have any inspecting functions for the data tables below. I believe the step-by-step process I walked you through previously is sufficient evidence that I constantly check my data after every step to make sure everything is okay.

<br>

#### Pacific Region
**Alaska**
```{r, message = FALSE}
Alaska <- read_excel(path = xls_data, sheet = "Alaska", na = "--")

names(Alaska)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Alaska <-
  Alaska %>%
  filter(row_number() > 5, row_number() < 165)

Alaska <-
  Alaska %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Alaska <-
  Alaska[-c(2, 4, 5, 6, 12, 15, 17, 20, 28, 32, 44, 50, 58, 59, 71, 84, 103, 105, 144, 155),]

Alaska <-
  Alaska %>%
  na.omit()

Alaska <-
  Alaska %>%
  mutate(state = "Alaska")


Alaska <-
  Alaska %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Alaska)
```

**Hawaii**
```{r, message = FALSE}
Hawaii <- read_excel(path = xls_data, sheet = "Hawaii", na = "--")

names(Hawaii)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Hawaii <-
  Hawaii %>%
  filter(row_number() > 5, row_number() < 158)

Hawaii <-
  Hawaii %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Hawaii <-
  Hawaii[-c(2, 4:6, 11, 13, 16, 20, 28, 31, 42, 50, 61, 62, 76, 88, 122, 124, 141, 149),]

Hawaii <-
  Hawaii %>%
  na.omit()

Hawaii <-
  Hawaii %>%
  mutate(state = "Hawaii")


Hawaii <-
  Hawaii %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Hawaii)
```

**Utah**
```{r, message = FALSE}
Utah <- read_excel(path = xls_data, sheet = "Utah", na = "--")

names(Utah)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Utah <-
  Utah %>%
  filter(row_number() > 5, row_number() < 169)

Utah <-
  Utah %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Utah <-
  Utah[-c(2, 4:6, 12, 14, 17, 22, 29, 33, 44, 52, 64, 65, 79, 91, 115, 117, 141, 155),]

Utah <-
  Utah %>%
  na.omit()

Utah <-
  Utah %>%
  mutate(state = "Utah")


Utah <-
  Utah %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Utah)
```

<br>

#### Midwest Region
**Iowa**
```{r, message = FALSE}
Iowa <- read_excel(path = xls_data, sheet = "Iowa", na = "--")

names(Iowa)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Iowa <-
  Iowa %>%
  filter(row_number() > 5, row_number() < 161)

Iowa <-
  Iowa %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Iowa <-
  Iowa[-c(2, 4:6, 12, 14, 18, 22, 30, 34, 45, 54, 64, 65, 78, 91, 108, 110, 133, 149),]

Iowa <-
  Iowa %>%
  na.omit()

Iowa <-
  Iowa %>%
  mutate(state = "Iowa")


Iowa <-
  Iowa %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Iowa)
```

**Michigan**
```{r, message = FALSE}
Michigan <- read_excel(path = xls_data, sheet = "Michigan", na = "--")

names(Michigan)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Michigan <-
  Michigan %>%
  filter(row_number() > 5, row_number() < 185)

Michigan <-
  Michigan %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Michigan <-
  Michigan[-c(2, 4:7, 13, 15, 18, 23, 31, 35, 48, 62, 76, 77, 91, 111, 132, 134, 157, 172, 175),]

Michigan <-
  Michigan %>%
  na.omit()

Michigan <-
  Michigan %>%
  mutate(state = "Michigan")


Michigan <-
  Michigan %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Michigan)
```

**Ohio**
```{r, message = FALSE}
Ohio <- read_excel(path = xls_data, sheet = "Ohio", na = "--")

names(Ohio)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Ohio <-
  Ohio %>%
  filter(row_number() > 5, row_number() < 187)

Ohio <-
  Ohio %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Ohio <-
  Ohio[-c(2, 4, 5, 6, 11, 14, 18, 22, 30, 34, 47, 60, 73, 74, 89, 107, 124, 126, 155, 171),]

Ohio <-
  Ohio %>%
  na.omit()

Ohio <-
  Ohio %>%
  mutate(state = "Ohio")


Ohio <-
  Ohio %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Ohio)
```

<br>

#### South Region
**Texas**
```{r, message = FALSE}
Texas <- read_excel(path = xls_data, sheet = "Texas", na = "--")

names(Texas)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Texas <-
  Texas %>%
  filter(row_number() > 5, row_number() < 228)

Texas <-
  Texas %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Texas <-
  Texas[-c(2, 4:7, 13, 16, 20, 24, 32, 36, 48, 63, 66, 80, 81, 95, 117, 145, 147, 157, 192, 210, 213),]

Texas <-
  Texas %>%
  na.omit()

Texas <-
  Texas %>%
  mutate(state = "Texas")


Texas <-
  Texas %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Texas)
```

**Florida**
```{r, message = FALSE}
Florida <- read_excel(path = xls_data, sheet = "Florida", na = "--")

names(Florida)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Florida <-
  Florida %>%
  filter(row_number() > 5, row_number() < 200)

Florida <-
  Florida %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Florida <-
  Florida[-c(2, 4:7, 13, 16, 20, 25, 33, 37, 50, 63, 76, 77, 91, 109, 132, 134, 166, 181),]

Florida <-
  Florida %>%
  na.omit()

Florida <-
  Florida %>%
  mutate(state = "Florida")


Florida <-
  Florida %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Florida)
```

**Virginia**
```{r, message = FALSE}
Virginia <- read_excel(path = xls_data, sheet = "Virginia", na = "--")

names(Virginia)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Virginia <-
  Virginia %>%
  filter(row_number() > 5, row_number() < 194)

Virginia <-
  Virginia %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Virginia <-
  Virginia[-c(2, 4:6, 12, 14, 17, 22, 31, 35, 48, 61, 75, 76, 90, 108, 131, 133, 160, 178),]

Virginia <-
  Virginia %>%
  na.omit()

Virginia <-
  Virginia %>%
  mutate(state = "Virginia")


Virginia <-
  Virginia %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Virginia)
```

<br>

#### Northeast Region
**Pennsylvania**
```{r, message = FALSE}
Pennsylvania <- read_excel(path = xls_data, sheet = "Pennsylvania", na = "--")

names(Pennsylvania)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Pennsylvania <-
  Pennsylvania %>%
  filter(row_number() > 5, row_number() < 191)

Pennsylvania <-
  Pennsylvania %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Pennsylvania <-
  Pennsylvania[-c(2, 4:6, 12, 14, 17, 21, 29, 33, 47, 60, 74, 75, 89, 107, 129, 131, 158, 164, 176, 179),]

Pennsylvania <-
  Pennsylvania %>%
  na.omit()

Pennsylvania <-
  Pennsylvania %>%
  mutate(state = "Pennsylvania")


Pennsylvania <-
  Pennsylvania %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Pennsylvania)
```

**Vermont**
```{r, message = FALSE}
Vermont <- read_excel(path = xls_data, sheet = "Vermont", na = "--")

names(Vermont)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Vermont <-
  Vermont %>%
  filter(row_number() > 5, row_number() < 120)

Vermont <-
  Vermont %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Vermont <-
  Vermont[-c(2, 4:6, 11, 13, 16, 20, 27, 31, 42, 49, 59, 60, 71, 83, 90, 92, 101, 109),]

Vermont <-
  Vermont %>%
  na.omit()

Vermont <-
  Vermont %>%
  mutate(state = "Vermont")


Vermont <-
  Vermont %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Vermont)
```

**Maine**
```{r, message = FALSE}
Maine <- read_excel(path = xls_data, sheet = "Maine", na = "--")

names(Maine)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Maine <-
  Maine %>%
  filter(row_number() > 5, row_number() < 133)

Maine <-
  Maine %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Maine <-
  Maine[-c(2, 4:6, 11, 13, 16, 20, 27, 31, 42, 50, 62, 63, 74, 84, 93, 95, 110, 121),]

Maine <-
  Maine %>%
  na.omit()

Maine <-
  Maine %>%
  mutate(state = "Maine")


Maine <-
  Maine %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Maine)
```

<br>

### Append the Tables

Now that I have the individual data tables for states, I will start appending them together in terms of region.   
From this wikipedia page ("https://en.wikipedia.org/wiki/File:Census_Regions_and_Division_of_the_United_States.svg"), I found a map of the regions and divisons that the Census utilizes for the United States.

**Pacific Region:** Only Alaska, Hawaii, and Utah
```{r}
Pacific <- bind_rows(Alaska, Hawaii, Utah) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(Pacific)
```

**Midwest Region:** Only Iowa, Michigan, and Ohio
```{r}
Midwest <- bind_rows(Iowa, Michigan, Ohio) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(Midwest)
```

**South Region:** Only Texas, Florida, and Virginia
```{r}
South <- bind_rows(Texas, Florida, Virginia) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(South)
```

**Northeast Region:** Only Pennsylvania, Vermont, and Maine
```{r}
Northeast <- bind_rows(Pennsylvania, Vermont, Maine) %>%
  arrange(desc(NumberSpeakers))

# Check out my new data table for the Pacific Region
head(Northeast)
```

<br>

### Other Data Source

This is the other data source I am using. It is also from the Census Bureau, but instead of language data for each state, it is for the nation as a whole. The data for each state would count as a subset of this data, so I wanted to know if the patterns I observe for the states/regions are also reflected in the overall nation data.
```{r, message = FALSE}
# Specifying Data Path (change path as needed since this is specific to where the data is saved on my laptop)
xls_nationData <- "C:/Users/GuaiGuai/Documents/R/STAT184_FinalProject/2009-2013-acs-lang-tables-nation.xls"

# This returns the names of all the sheets in our xls file (for future reference).
excel_sheets(path = xls_nationData)

# Name the data
Nation <- read_excel(path = xls_nationData, na = "--")

# Let's take a look at the data
head(Nation)
```

The data doesn't look too great, so we need to do some cleaning. I followed the same process I used for cleaning the states data tables, except I don't need the mutate() function to add a new variable for state since this data table is for the entire nation (every state and territory in the United States of America).
```{r}
names(Nation)[c(1, 2, 3, 4, 5)] <- c("Language", "NumberSpeakers", "SpeakersError", "EnglishNotVeryWell", "EnglishError")

Nation <-
  Nation %>%
  filter(row_number() > 5, row_number() < 377)

Nation <-
  Nation %>% replace_with_na(replace = list(NumberSpeakers = c("(D)", "(B)", "(X)"), SpeakersError = c("(D)", "(B)", "(X)"), EnglishNotVeryWell = c("(D)", "(B)", "(X)"), EnglishError = c("(D)", "(B)", "(X)")))

Nation <-
  Nation[-c(2, 4:8, 14, 17, 21, 26, 35, 39, 53, 68, 89, 90, 106, 132, 175, 177, 331, 352, 356),]

Nation <-
  Nation %>%
  na.omit()


Nation <-
  Nation %>%
  mutate(NumberSpeakers = as.numeric(NumberSpeakers), SpeakersError = as.numeric(SpeakersError), EnglishNotVeryWell = as.numeric(EnglishNotVeryWell), EnglishError = as.numeric(EnglishError))

str(Nation)
```

<br>

## Brief Description & Exaplanation Of the Data & the Research
I knew I wanted to do research and analysis on languages, culture, or something in the medical field, but I decided to focus my search on languages. I was able to find this data from the Census Bureau (specifically this website: "https://www.census.gov/data/tables/2013/demo/2009-2013-lang-tables.html
"). It contains four separate xls documents that each contain multiple sheets of data. It is all describing detailed languages spoken at home and ability to speak English for the population over a 5 year period (from 2009 to 2013).   

From the description on the site, I saw that the the tables are available for the nation, each of the 50 states, plus Washington, D.C. and Puerto Rico, counties with 100,000 or more total population and 25,000 or more speakers of languages other than English & Spanish, as well as core-based statistical areas (metropolitan statistical areas & micropolitan statistical areas) with 100,000 or more total population and 25,000 or more speakers of langauges other than English & Spanish.   

The information is collected by the American Community Survey, which contains multi-year data used to list all languages spoken in the United States that were reported during the sample period. The data is maintained by the Census Bureau's application programming interface (API).

Unfortunately, the data is only a sample of the total population since the ACS did not sample the households where some |   other languages are spoken, or because the person filling out the survey didn't report some languages and/or possibiliity reported another language instead. The English-speaking ability variable is self-reported so it represents the person's own perception about his or her own ability. The ACS questionnaires are also usually completed by one household member, so it may not reflect the overall household.   


For the data I am using (the states), there are separate tables for each state. A case is usually represented by a single language (or a group of languages to generalize some more specific languages) in each state. The cases vary by the state, but since I am planning on appending the tables to each other, we can do some rough math. If there are 52 tables for states and each table has about 145 cases (languages), and if we append all those tables together, we'll get around 7,540 caes in total. Unfortunately, as you can see, I only appended 3 states for each region (northeast, west, south, midwest, etc.) and so we have separate tables for each region instead.   

Now that I am done cleaning and processing the data, there are 6 variables, language (categorical), number of speakers (numerical), margin of error (numerical), speak English less than "very well" (numerical), second margin of error (numerical), and state (categorical). I plan to mainly focus on the language, number of speakers, speak English less than "very well", and state. I want to see if there are any patterns in the language being spoken affecting the English speaking ability (maybe by how similar the language is to English). I also want to compare these trends between states (or most likely regions) and see if there are similar trends or not.   

I will also be trying to wrangle my data into a method so that I can filter for only a couple languages so that I can plot separate lines for each language based on total speakers and states. This will depend on how data wrangling will go, which you will see in the next section.

In combination with my other data source, the nation data, I want to see if the same pattern seen in states and/or regions are seen in the overall nation as well. The nation data contains the same variables as the state data. One aspect I didn't mention earlier is the fact that the languages that show up for each state (and the nation) may differ slightly. There will be a good majority of languages that are the same across the board, but some more unique languages may not show up in every data table.

<br>

## Data Wrangling
Before we dive into data wrangling, let's take a look at some summary statistics.

### Summary Statistics
So first off, I ran the summary() and glimpse() functions on the Pacific Region data to get a brief glimpse of what's going on with the data. Remember that each region I created only contain 3 out of all the states in that region due to how much hard-coding and cleaning I would have to do if I imported all 52 tables for the states plus Puerto Rico.
```{r}
summary(Pacific)
```
The summary function tells us that there are 204 total cases (because of the 206 languages). Minimum number of speakers for a language is 4 people, but the maximum is 245947 people, with an average of 3881.4 people per language.   
Just as a quick note, the margin of errors are the 90% error.

```{r}
glimpse(Pacific)
```
Using the glimpse() funciton, we get the number or rows, number or columns, the variable names, and a brief look at what kind of data is under each variable. For my Pacific data table, there are 204 rows/cases and 6 columns/variables. In my preliminary EDA, I had noticed there were periods in front of my language names. After discussing with the TA, I ended up taking those out manully within the excel spreadsheet (basically hard-coding the data) so that I wouldn't have to worry about taking it out in R Studio.

Now, let's take a look at each of the other regions. Remember, each of my regions only contains 3 states, so this cannot be generalized to the entire region. I would have to clean and use the data for every single state in the region for us to get a better idea of the patterns/relationships. Also note that I excluded every language that had at least one NA value to make wrangling and graphing easier for this project.   
I only used the summary() function since you get the general gist of what glimpse() does and there's no need to use that function for every other region as well.
```{r}
summary(Midwest)
```
There are 271 total cases, meaning 271 languages after my cleaning for the Midwest region. The minimum number of speakers is 35 and the maximum is 270708, with an average of 6500 people per language.

```{r}
summary(South)
```
The total number of cases, i.e. the total number of languages, is 327 for the South region. The minimum number of speakers is 10, the maximum is 6983384, and the average is 43713.

```{r}
summary(Northeast)
```
For the Northeast region, there are 190 total cases/number of languages. The minimum number of speakers is 4, maximum is 525218, and the average is 7059.9.

After looking at the regions, let's take a look at the overall nation. I decided to use both summary() and glimpse() since this dataset is a bit different than the previous datasets for the regions.
```{r}
summary(Nation)
```
We can see that there are 262 total cases/number of languages for the entire nation. The minimum number of speakers is 35, maximum is 37458624, and the average is 230367. Please remember that I took out all languages had at least one NA value to make it easier for processing and wrangling/visualizations.

```{r}
glimpse(Nation)
```
As you can, the variables are the same for the region data and you get a look at some of the values in each variable. The only missing variable/column is that there is no state variable since this is for the entire nation.


### **Now let's get started with data wrangling.**   
I started off with a simple query that I had an issue getting to work properly in my preliminary EDA. I basically take my Pacific data table and group it by the language, then summarize it to sum up the total number of speakers for each langauge. 
Based on the output below, my code works!   
As you can see, my output is arranged in alphabetical order by language name, even though I didn't specify anything with the arr() function.
```{r, message = FALSE}
PacificTotal <-
  Pacific %>%
  group_by(Language) %>%
  summarize(Pacific = sum(NumberSpeakers))

# Check the output
PacificTotal
```

Let's do the same thing for the other regions so that I can join the tables later.
```{r, message = FALSE}
MidwestTotal <-
  Midwest %>%
  group_by(Language) %>%
  summarize(Midwest = sum(NumberSpeakers))

# Check the output
MidwestTotal
```
```{r, message = FALSE}
SouthTotal <-
  South %>%
  group_by(Language) %>%
  summarize(South = sum(NumberSpeakers))

# Check the output
SouthTotal
```
```{r, message = FALSE}
NortheastTotal <-
  Northeast %>%
  group_by(Language) %>%
  summarize(Northeast = sum(NumberSpeakers))

# Check the output
NortheastTotal
```

Now that we have all the total number of speakers for each region, we can join the tables for an overall total.   
Let's start off just joining the PacificTotal table with the MidwestTotal table with a full join and checking the output before continuing.
```{r, message = FALSE}
# I used a full join and joined by the variable Language
Total <-
  full_join(PacificTotal, MidwestTotal, by = "Language")

# Check the output
Total
```

Everything look fine, so let's keep joining tables until we get the entire table we want.
```{r}
Total <-
  full_join(Total, SouthTotal, by = "Language")

Total <-
  full_join(Total, NortheastTotal, by = "Language")

# I created a new data table for the total in the nation data table so that I can join it to my Total table
NationTotal <-
  Nation %>%
  summarize(Language, NationTotal = NumberSpeakers)

# Last full_join
Total <-
  full_join(Total, NationTotal, by = "Language")

# Take a look at the final resulting table
Total
```
This data table looks pretty interesting and I think I could use it to output some pretty interesting visualizations in the next section.

But before then, I want to try some more data wrangling to get some interesting data tables. Let's use the **spread()** function.
```{r}
# I created a new data table called SouthSum that only included the Language, NumberSpeakers, and state variables.
SouthSum <-
  South %>%
  select(Language, NumberSpeakers, state)

# Then using the new data table, I spread the data into a wide format so that I had Florida, Texas, and Virginia as variables and the values in each column represents the number of speakers in that state for the language. So each row/case is a specific language and the total number of speakers for that language in each of the 3 states.
SouthSum <-
  spread(SouthSum, key = state, value = NumberSpeakers)

SouthSum
```
This is similar to my Total table, but this is specific for just the three states in the South region.

Now, I've been curious about certain patterns for the languages so I'm going to use regex expressions to find those patterns. Unfortunately, this is not something I'd like to visualize, I was just curious to see if it might give me any insight into the data.
```{r}
# I used grep() to identiy which Languages have a number/digit at the end of it's name in the Nation data table and received two languages.
grep("[[:digit:]]$", Nation$Language, value = TRUE)
```
```{r}
# This time, I used grep() to identiy which Language names contained a space of some sort. As you can see, there are quite a few languages that follow that pattern.
grep("\\s", Nation$Language, value = TRUE)
```
Like I stated before, this information unfortunately doesn't give me any insight into the data and my research question, but I am able to satiate my curiousity of how many language names match these patterns.

<br>

## Data Visualization

### Plots
Let's first see if there's a relationship between the number of speakers of a language and their English ability. I created a simple dot plot with a line that helps us recognize any patterns in the data for the Pacific Region. I plotted the Number of Speakers versus the English Ability variable and differentiated color based on the state. As you can see from the output, if we have more than three states, then this plot could become very interesting. If the plot becomes too messy looking, we could even use faceting instead to differentiate the states.   
From this plot, we can see that for the Pacific Region, as the number of speakers of a language increases, so does the amount of people who claim their English speaking ability is not/less than "Very Well".
```{r}
# The color aeesthetic is by the state variable and I overlayed geom_point() with geom_smooth(). I also added x and y-axis labels and a title for the visual
Pacific %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```

But it's hard to see the data for Alaska since Utah has a huge outlier, so what if we limited the x-axis and y-axis ranges to take out the outlier?
```{r}
# Here I just added limitaitons for the x and y-axis value ranges. The x-axis is limited to 0 to 60000 and the y-axis is limited to 0 to 40000. This takes out the extreme data point (maybe an outlier but maybe not) that we saw earlier.
Pacific %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlim(0, 60000) +
  ylim(0, 40000) +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```
So now that we can see the Alaska and Hawaii data more clearly, we see that there's a general positive trend, meaning the greater the number of speakers, the more people claim their English ability is poor. Unfortunately, it seems like there's a greater error range for Hawaii than Alaska and Alaska's curve actually starts curving down at the end. So can we conclude that the higher the number of speakers of a language, the more number of peopel who claim their English ability is poor? We need to look at more visualizations.

What about for the other regions?
```{r}
Midwest %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```
For the Midwest region, the lines are clearer and show an evident positive relationship, but let's check the other two regions as well.

```{r}
South %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```
It seems that the error range is much larger for Texas than any other states we've seen so far. It's a bit concerning, although all three states still follow the positive relationship we saw earlier.

I decided to limit the ranges to show a clearer image and the pattern differences. It's very interesting to look at, but each seems to all hold a positive relationship, just some not as steep as others.
```{r}
South %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region") +
  xlim(0, 90000) +
  ylim(0, 40000)
```

Let's take a look at the last region, Northeast.
```{r}
Northeast %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell, color = state)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```
Although I can't quite see the Vermont data and the Maine data seems to be positive and then plateaus, the Pennsylvania shows a clear positive relationship with a small error range. I think from all these graphs we saw, it may be safe to safe that there is indeed a positive relationship between the number of speakers of a language and the number of people of that language who claim their English ability is poor.

Let's see if the same pattern is apparent in the overall nation data.
```{r}
Nation %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region")
```
Although the error range is slightly larger than some individual states, overall, there seems to be a somewhat positive relationship, but I'd like to see what will happen if I limit the x and y-axis ranges.
```{r}
Nation %>%
  ggplot(aes(x = NumberSpeakers, y = EnglishNotVeryWell)) +
  geom_point() +
  geom_smooth() +
  xlab("# of Speakers") +
  ylab("# of People Who Claim Their English Ability is Poor") +
  labs(title = "Speakers vs English Ability in the Pacific Region") +
  xlim(0, 100000) +
  ylim(0, 70000)
```
Once we zoomed in on the graph, we see a very interesting pattern. The datapoints give us a pattern that's almost parabolic. It's still a positive relationship, but much different than what we expected from when we included all the datapoints.

I'd like to see how the number of speakers differ for each language in each region to see if there might be something here that could relate to the positive relationship we discovered above. We select a random selection of 5 languages that include datapoints for all regions.
```{r}
# I had to use gather() to change my data table to a longer format before I could output a graph that I wanted.
# I then graphed a density plot for the total number of speakers for each language with the line colours differentiating based on region.
Total %>%
  gather(Pacific, Midwest, South, Northeast, NationTotal, key = Region, value = SpeakerTotal) %>%
  ggplot(aes(SpeakerTotal, colour = Region, fill = Region)) +
  geom_density(alpha = 0.1) +
  xlim(0, 20000)
```
This output is actually very interesting to see because it shows that the density for smaller number of speakers is much higher than everything else. The higher the number of speakers goes, the smaller the density. I think this is important to know because it affects how we can ultimately interpret the graphs we saw above with the positive relationships, especially when we included all the data points. It shows how easily an "outlier" datapoint for a large number of speakers could affect the plot, therefore, zooming in (like what I did for a couple graphs), actually really helps us see the patterns more clearly.

Overall, the relationships are still all positive, but when we zoomed in, we saw how much they could differentiate (whether it's based on steepness or if it almost looks like a parabolic curve, or even plateaus, etc.).

<br>

## Conclusion/Final Thoughts




